{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLM Engine","text":""},{"location":"#what-is-llm-engine","title":"What is LLM Engine?","text":"<p>LLM Engine is an extensible service for deploying and developing assistive interactive agents in multi-modal, intentional conversations, both online and hybrid.</p> <p>\u2705 LLM Engine intentionally supports events and educational settings</p> <p>\u2705 LLM Engine connects to a variety of familiar and novel conversational frontend clients.</p> <p>\u2705 LLM Engine can be used as a backend for production agent deployments or for the development and evaluation of new agents.</p> <p>\u2705 LLM Engine support deploying LLMs in multi-user, multi-turn environments. To our knowledge, it is the only extensible service that does so!</p> <p>\u2705 LLM Engine is at its heart a flexible conversational messaging platform with easy integration of LLM agents to evaluate, respond and contribute.</p> <p>\ud83d\ude80 Watch this demo video to see it in action!</p>"},{"location":"#who-is-llm-engine-for","title":"Who is LLM Engine for?","text":"<p>\ud83d\udc64 Teachers and online or hybrid event hosts who want to enhance their events with LLM agents to better achieve their goals.</p> <p>\ud83d\udee0\ufe0f Developers and evaluators of LLM agents for online and hybrid events and conversations.</p>"},{"location":"#production-agents","title":"Production Agents","text":"<p>LLM Engine comes with three tested agents that are ready for use in your events or classrooms*.</p> <p>\u2705 Back Channel Insights Agent - Synthesizes private messages sent by event participants into actionable insights for a moderator</p> <p>\u2705 Back Channel Metrics Agent -Aggregates quick event participant reactions into metrics sent to a moderator</p> <p>\u2705 Event Assistant - Answers questions related to an event in real-time or asynchronously, using the event transcript and/or conversation history</p> <ul> <li>NOTE: Additional experimental agents are available for evaluation and further development.</li> </ul>"},{"location":"#supported-clients","title":"Supported Clients","text":"<p>While any front end client could use our APIs, we specifically support the following:</p> <p>\u2705 Nextspace</p> <p>\u2705 Zoom (audio and chat, including DMs)</p>"},{"location":"#supported-llm-platforms-and-models","title":"Supported LLM Platforms and Models","text":"<p>LLM Engine works with a variety of LLM platforms and models:</p> <p>\u2705 OpenAI - any model</p> <p>\u2705 Claude - any model</p> <p>\u2705 AWS Bedrock - any model</p> <p>\u2705 Open Source Models - most models</p> <p>See our installation guide for more information.</p>"},{"location":"#installing","title":"Installing","text":"<p>To install and try out LLM Engine please see our installation guide and usage guide.</p>"},{"location":"#development","title":"Development","text":"<p>Developers can find additional technical documentation in our guide for developers.</p>"},{"location":"#code-repository","title":"Code repository","text":"<p>LLM Engine is open source! You can find our Github repository at https://github.com/berkmancenter/llm_engine.</p>"},{"location":"#credits","title":"Credits","text":"<p>LLM Engine is a project of the Applied Social Media Lab at Harvard University.</p>"},{"location":"developers/","title":"LLM Engine Developer Documentation","text":""},{"location":"developers/#developer-capabilities-and-features","title":"Developer Capabilities and Features","text":"<p>\u2705 Include multiple LLM agents in a voice or text conversation using our REST and Websocket APIs</p> <p>\u2705 Configure agents to intercept and respond to user messages immediately and/or execute periodically</p> <p>\u2705 Configure agents to receive and respond to messages on a variety of channels, including directly to a user.</p> <p>\u2705 Choose from a variety of agent templates and easily create new ones</p> <p>\u2705 Support for multiple LLM models including OpenAI and open-source models using Ollama, Runpod, and Modal. (More coming soon!)</p> <p>\u2705 Modify agent prompts, models, and RAG materials via API to create custom interventions</p> <p>\u2705 Support for plain text and structured data messages/payloads</p> <p>\u2705 Use with multiple simultaneous front-end clients such as Nextspace and Zoom in a single conversation</p> <p>\u2705 Access and save real-time and full event transcripts, from both text and audio interactions</p> <p>\u2705 Supports organizing events into topics. Events can have channels to segment a conversation.</p> <p>\u2705 JWT authentication and support for pseudonymous user interaction, user persistence and optional login.</p> <p>\u2705 Support for prompt experimentation, testing and reporting, including replaying previous events with revised prompts</p> <p>\u2705 Define and configure your own agents via API and customize conversation flow</p> <p>\u2705 Observe and evaluate agent performance using Langsmith and Promptfoo</p> <p>\u2705 Support threaded messages</p> <p>[ ] LLM automated prompt iteration and refinement</p>"},{"location":"developers/#system-description","title":"System Description","text":"<ol> <li>Conversations are organized into \"Topics\"</li> <li>Conversations have many sequential Messages.</li> <li>Messages can contain structured data and be threaded through a parent/child relationship between Messages.</li> <li>Messages can be received or sent through different Adapters connected to various transports (e.g. Websockets or HTTP API) and platforms (e.g. Zoom, Slack or Nextspace).</li> <li>Messages can be sent or received through one or more Channels to segregate different audiences (e.g. a moderator vs. participants) or content (e.g. group messages, DMs or audio transcripts)</li> <li>Agents can    a. Intercept each message or be scheduled to run periodically    b. Evaluate Conversation history as desired and respond or reject user messages.    c. Make use of RAG if desired</li> <li>Conversations can be replayed with different Agent configurations as Experiments to learn and iterate Agent designs.</li> <li>Benchmarks can be developed to evaluate Agents on different dimensions.</li> </ol> <p>Note: Since messages allow for structured data they can act as a transaction log, supporting a wide variety of use contexts beyond messaging apps (e.g. also gaming, real-time collaboration tools). This transaction log also enables agent replay and experiments.</p>"},{"location":"developers/#system-diagram","title":"System Diagram","text":"<p>Below is a diagram of the primary system components and how they are connected.</p> <pre><code>flowchart TD\n  subgraph LLM Engine\n    direction TB\n    Topic[\"Topic\"]\n    Conversation[\"Conversation\"]\n    Experiment[\"Experiment\"]\n    Benchmark[\"Benchmark\"]\n    Agent[\"Agent\"]\n    Channel[\"Channel\"]\n    Message[\"Message*\"]\n    Thread[\"Thread\"]\n    Zoom[\"Zoom (Audio/Text)\"]\n    Nextspace[\"Nextspace\"]\n    Prompts[\"Prompts\"]\n    MCPTools[\"MCP Tools\"]\n    MCPResources[\"MCP Resources\"]\n    Slack[\"Slack\"]\n    API[\"HTTP API\"]\n\n    Topic --&gt;|many| Conversation\n    Conversation --&gt;|many| Agent\n    Conversation --&gt;|many| Channel\n    Conversation --&gt;|many| Experiment\n    Channel &lt;--&gt;|many| Message\n    Channel -. \"optional\" .-&gt; Conversation\n\n    Prompts --&gt; Agent\n    Benchmark --&gt;|evaluate| Agent\n    RAG --&gt; Agent\n    Agent -. \"planned\" .-&gt; MCPTools\n    Agent -. \"planned\" .-&gt; MCPResources\n\n    Message --&gt; |intercept| Agent\n    Agent --&gt;|create| Message\n\n    Message --&gt; |parent/children|Thread\n\n    Message &lt;--&gt;|many|Adapter\n\n    Adapter &lt;--&gt; Websocket\n    Adapter &lt;--&gt; API\n    Adapter &lt;--&gt; Nextspace\n    Adapter &lt;--&gt; Slack\n    Adapter &lt;--&gt; Zoom\n\n    end\n\n  ExternalAgents[\"External Agents\"]\n  ExternalAgents &lt;-. \"A2A protocol (planned)\" .-&gt; Agent\n</code></pre>"},{"location":"developers/#repofolder-overview","title":"Repo/Folder overview","text":"<ul> <li>docs - Documentation files, including this file!</li> <li>k6 - Load testing</li> <li>promptfoo - Promptfoo safety testing configuration</li> <li>rag_documents - Saved documents used with some agents</li> <li>report_templates - Templates used for generating reports</li> <li>src - Main source code folder</li> <li>adapters - Adapters for different communication platforms (e.g. Zoom, Slack)</li> <li>agents - Conversational agent configuration and code</li> <li>config - Main app config</li> <li>controllers - API route controllers</li> <li>conversations - Conversations config</li> <li>docs - OpenAPI docs helpers</li> <li>handlers - Various handlers</li> <li>jobs - Scheduled job handlers</li> <li>middlewares - expressjs middleware</li> <li>models - mongoose/mongodb data models</li> <li>routes - api route definitions</li> <li>services - core services used by controllers and elsewhere</li> <li>types - type definitions</li> <li>utils - various utilities</li> <li>validations - API validations</li> <li>websockets - websocket handlers</li> <li>tests - Extensive automated tests</li> </ul>"},{"location":"developers/#api-documentation","title":"API documentation","text":"<p>OpenAPI documentation is available when running the app locally at the route <code>v1/docs</code>.</p> <p>In particular, see the examples in the <code>POST</code> <code>Conversations</code> endpoint for how to create conversations with certain agent types.</p>"},{"location":"developers/#understanding-message-control-flow","title":"Understanding Message Control Flow","text":"<p>It can be very helpful to understand the Message routing and control flow to and from Agents and different platforms through Adapters.</p>"},{"location":"developers/#diagrams","title":"Diagrams","text":"<p>You can find several diagrams to help you understand the message routing and control flow.</p> <p>Received Message Routing to Agents Describes how new messages are processed and routed through different agents for action.</p> <p>Agent Evaluate and Respond Describes how agents evaluate and choose to respond to or reject received messages.</p> <p>Agents Receive a Message from Nextspace or Nymspace A more detailed description of what happens when a message is received from a websocket-based web app like Nextspace or Nymspace.</p> <p>Agents Receive a Message from Zoom or Slack A more detailed description of what happens when a message is received from an external app like Zoom or Slack.</p> <p>New Users via an Adapter Describes how new system users are created as needed on demand when they encountered through an Adapter</p>"},{"location":"developers/#tests-and-benchmarks","title":"Tests and Benchmarks","text":""},{"location":"developers/#unit-and-integration-tests","title":"Unit and integration tests","text":"<p>Unit and integration tests can be run with <code>yarn run test</code>, and a coverage report can be seen with <code>yarn run coverage</code>.</p>"},{"location":"developers/#agent-tests","title":"Agent tests","text":"<p>Tests of the built-in \"production\" agent types can be run with <code>yarn test:agents</code>.</p>"},{"location":"developers/#red-teaming","title":"Red teaming","text":"<p>We integrate with Promptfoo for red teaming our agents. To run a red team against the Event Assistant agent, execute <code>yarn redteam:event-assistant.</code> You will be asked to provide an email address. You do not need to create an account with Promptfoo. Any valid email address will work to run the test and report the results locally. NOTE: the default redteam configuration runs several plugins with an interative jailbreak strategy. We have observed test runs taking almost two hours to complete and costing approximately $20 for LLM usage with a GPT-4o-mini model.</p> <p>This target uses a custom Promptfoo provider, located at <code>tests/utils/promptfooProvider.ts</code>, to create a simulated event and pass questions generated by Promptfoo to the Event Assistant for response. This provider uses the settings you have specified in your <code>.env</code> file to connect to Mongo, specify the LLM provider and model, etc. It essentially simulates running the agent as it runs in LLM Engine.</p> <p>The promptfoo configuration file, located at <code>promptfoo/promptfoconfig-event-assistant.yaml</code> provides information about the type of event we are simulating (a talk about why part-time work is better) under <code>redteam:purpose</code>. This information allows Promptfoo to tailor its iterative prompting to the test scenario, asking relevant questions about the simulated event in its jailbreak attempts.</p> <p>You can duplicate and/or modify the provider code and configuration file to allow for red teaming additional agents.</p>"},{"location":"developers/#production-vs-development-agents","title":"Production vs. Development Agents","text":"<p>A few agent types have been carefully tested and vetted for \"production\" use. There are other experimental development agent types under development in the <code>agents/development</code> folder. Be careful running development agents! They should only be used in experimental or development contexts.</p>"},{"location":"developers/#enabling-development-agents","title":"Enabling development agents","text":"<p>In your <code>.env</code> file set <code>ENABLE_DEVELOPMENT_AGENTS=true</code>.</p>"},{"location":"developers/#testing-development-agents","title":"Testing development agents","text":"<p>To run tests of the built-in development agent types, run <code>yarn test:agents:development</code>.</p>"},{"location":"developers/#production-vs-development-platforms","title":"Production vs. Development Platforms","text":"<p>A few platforms types have been carefully tested and vetted for \"production\" use (e.g. Nextspace and Zoom). There are other experimental development platforms under development. New platforms can be added by create a new adapter. Be careful with development platforms! They should only be used in experimental or development contexts.</p>"},{"location":"developers/#enabling-development-platforms","title":"Enabling development platforms","text":"<p>In your <code>.env</code> file set <code>ENABLE_DEVELOPMENT_ADAPTERS=true</code>.</p>"},{"location":"developers/#openapi-specs-and-types","title":"OpenAPI specs and types","text":"<p>The server exposes our OpenAPI spec at <code>/v1/openapi.json</code>. You can use the <code>package.json</code> script <code>openapi-types:generate</code> to generate a corresponding types file which is placed at <code>src/types/openapi.d.ts</code>. Both the backend and the frontend can use this OpenAPI route and this script to generate type files, instead of having to important them as a separate package. Nice!</p>"},{"location":"developers/adapters/","title":"Adapters","text":""},{"location":"developers/adapters/#adapters","title":"Adapters","text":"<p>LLM Engine can be connected to external systems through the use of adapters and their associated webhooks. We currently provide two adapters: Zoom and Slack. This guide explains how to add more!</p> <p>There are two components to an adapter: the adapter itself and the webhook handler that routes communication to the adapter.</p> <p>While we have a WebSocket interface to LLM Engine, we do not yet support using it to communicate with external systems through adapters.</p>"},{"location":"developers/adapters/#define-an-adapter","title":"Define an Adapter","text":"<p>Adapters contain the core functionality for sending messages to and receiving messages from the external system. They essentially act as translators of both <code>Message</code> and <code>User</code> information and they also contain the logic needed to send a message to an external system or to establish an initial connection.</p> <p>Follow the steps below to add a new adapter.</p> <ol> <li>Create the file <code>src/handlers/[adapterName].ts</code></li> <li>Implement the following methods, specific to the external system's data formats. See our Zoom and Slack adapters for examples.</li> </ol> <p>NOTE: any properties specific to an individual <code>Conversation</code> can be stored in the Adapter's <code>config</code> property for reuse.</p> <pre><code>  start()  // optional logic to execute when conversation starts\n\n  stop()  // optional logic to execute when conversation stops\n\n  // translates received messages to appropriate format for LLM Engine to process\n  receiveMessage(message: Record&lt;string, unknown&gt;): AdapterMessage[]\n\n  sendMessage(message: IMessage) // sends message to external system\n\n  validateBeforeUpdate() // optional validation needed when conversation is created\n\n  // optional code to translate participant information into an LLM Engine user\n  participantJoined(participant: Record&lt;string, unknown&gt;): AdapterUser\n</code></pre> <ol> <li>Add your adapter to <code>src/adapters/index.ts</code></li> </ol>"},{"location":"developers/adapters/#define-a-webhook-handler","title":"Define a Webhook Handler","text":"<p>All incoming requests to LLM Engine should go through a webhook with URL in the format: [baseURL]/webhooks/[adapterName], where <code>adapterName</code> reflects the name of the external system (i.e. Zoom, Slack, etc.)</p> <p>The <code>webhookController</code> routes all webhook calls to the appropriate handler for the calling system. Follow the steps below to add a new webhook handler.</p> <ol> <li>Create the file <code>src/handlers/[adapterName].ts</code></li> <li>Add the following method, which should call <code>webhookService</code> to process valid events. Currently we support events of type <code>receiveMessage</code> and <code>participantJoined</code>.</li> </ol> <pre><code>const handleEvent = async (req, res) =&gt; {\n   // process the request payload specific to your system\n   // ultimately you need to find the Adapter through a unique\n   // set of options specific to your system\n\n   // This example uses botId from payload and an internal\n   // conversation ID configured as a URL query param\n   const conversation = await Conversation.findOne({ _id: conversationId }).populate('adapters').exec()\n   if (!conversation) {\n    throw new ApiError(httpStatus.NOT_FOUND, 'Conversation not found')\n   }\n   const botId = req.body.data.bot.id\n   const zoomAdapter = conversation.adapters?.find((adapter) =&gt;   adapter.type === 'zoom' &amp;&amp; adapter.config.botId === botId)\n  if (!zoomAdapter) {\n    throw new ApiError(httpStatus.NOT_FOUND, `No Zoom adapter with botId ${botId} configured for this conversation`)\n  }\n  if (event === 'transcript.data' || event === 'participant_events.chat_message') {\n    await webhookService.receiveMessage(zoomAdapter, req.body)\n  } else {\n    await webhookService.participantJoined(zoomAdapter, req.body.data.data.participant)\n  }\n  res.status(httpStatus.OK).send('ok')\n}\n</code></pre> <ol> <li>Add this method to perform any verification or authorization (we recommend doing so whenever possible). This will get called prior to invoking <code>handleEvent</code>.</li> </ol> <pre><code>const middleware = async (req, res, next) =&gt; {\n\n  try {\n    // validate signing secrets, URL tokens, etc.\n    next()\n  } catch (err) {\n    next(err)\n  }\n}\n</code></pre> <ol> <li>Add your handler to <code>src/handlers/index.ts</code></li> </ol>"},{"location":"developers/adapters/#try-it-out","title":"Try it Out","text":"<p>Create a <code>Conversation</code> that uses your new Adapter. This can be done through the <code>/conversations</code> endpoint. See OpenAPI documentation when running the app locally at the route <code>v1/docs</code> for details. The remainder of this section covers various payloads to use when creating the <code>Conversation</code>.</p> <p>All <code>Messages</code> in LLM Engine have one or more <code>Channels</code> defined that control access. Your Adapter must be configured to place messages received from the external system onto specific channels and/or send messages on specific channels to an external system.</p> <p>The <code>dmChannels</code>, <code>audioChannels</code>, and <code>chatChannels</code> adapter properties define how the adapter routes an LLM Engine message in a given Conversation.</p> <p>These channels should specify a Direction, either <code>incoming</code>, <code>outgoing</code>, or <code>both</code>. The default direction is <code>incoming</code>.</p> <ul> <li>Incoming: messages from the external system should be created in LLM Engine with these channels</li> <li>Outgoing: LLM Engine messages with these channels should be sent to the external system</li> <li>Both: these channels represent two-way communication</li> </ul> <p>** Be sure to include any channels you use in the <code>channels</code> property of the Conversation as well**</p> <p>If your external system supports transcription, configure the audioChannels property as below. NOTE: all transcripts must be placed on a channel named <code>transcript</code>. We may support additional channel names in the future.</p> <p>Example <code>Conversation</code> payload using transcription:</p> <pre><code>{\n    \"name\": \"Should plastic water bottles be banned?\",\n    \"topicId\": \"{{defaultTopic}}\",\n    \"channels\": [ { \"name\": \"transcript\"}],\n    \"adapters\": [ {\"type\": \"[adapterName]\", \"config\" : {\"customProp\": \"customValue\"}, \"audioChannels\": [{\"name\": \"transcript\"}]}]\n}\n</code></pre> <p>For chat, you can send/receive on group channels using <code>chatChannels</code></p> <p>Example <code>Conversation</code> payload using a general chat channel:</p> <pre><code>{\n    \"name\": \"Should plastic water bottles be banned?\",\n    \"topicId\": \"{{defaultTopic}}\",\n    \"channels\": [ { \"name\": \"participant\"}],\n    \"adapters\": [ {\"type\": \"[adapterName]\", \"config\" : {\"customProp\": \"customValue\"}, \"chatChannels\": [{\"name\": \"participant\", direction: \"both\"}]}]\n}\n</code></pre> <p>If you wish to enable direct messages between agents and users, you must set the <code>enableDMs\": [\"agents\"]</code> property during Conversation creation. You must also specify a <code>dmChannel</code> for each agent that should receive DMs.</p> <pre><code>{\n\"name\": \"Should plastic water bottles be banned?\",\n\"topicId\": \"{{defaultTopic}}\",\n\"enableDMs\": [\"agents\"]\n\"channels\": [ { \"name\": \"participant\"}],\n\"adapters\": [ {\"type\": \"[adapterName]\", \"config\" : {\"customProp\": \"customValue\"},\n    \"dmChannels\": [{\"direct\": true, \"agent\": \"eventAssistant\", \"direction\": \"both\"}]}]\n}\n</code></pre>"},{"location":"developers/development_agents/","title":"Development - Agents","text":"<p>\u2757 Caution! The agent types below are experimental! These are meant as inspiration for your own agents or for further evaluation and development. \u2757 Use them only with audiences who know they are engaging an experimental agent!</p> <p>They become available when enabled using the <code>ENABLE_DEVELOPMENT_AGENTS</code> environment variable.</p> <p>\u2705 Civility Agent - Preemptively intercepts toxic messages and suggests modifications</p> <p>\u2705 Delegates Agent - Creates a bots-only conversation between agents acting as delegates for participants and an \"expert\" moderator that uses RAG with topical reference material</p> <p>\u2705 Experts Agent - Creates a bots-only conversation between agents acting as experts with opposing viewpoints, using RAG with topical reference material</p> <p>\u2705 Playful Agent - Injects playful reactions and questions into a conversation</p> <p>\u2705 Radical Empathy Agent - Suggests modifications to messages to promote radical empathy among participants</p> <p>\u2705 Reflection Agent - Helps participants reflect on areas of agreement and disagreement</p> <p>[ ] Agenda Assistant - Helps a user to prepare a thoughtful, structured agenda conversationally</p>"},{"location":"developers/development_platforms/","title":"Development platforms","text":""},{"location":"developers/development_platforms/#development-supported-clients","title":"Development - Supported Clients","text":"<p>In addition to our two clients approved for production use (Nextspace and Zoom), we also support other platforms:</p> <p>\u2757 Caution! The platform types below are experimental! They are meant for evaluation, development and experimentation. \u2757 Use them only with audiences who know they are engaging on an experimental platform!</p> <p>\u2705 Nymspace - Visit the project for installation instructions</p> <p>\u2705 Slack (channels and DMs) - installation guide</p> <p>\u2705 Your own! Additional platforms can be supported by creating a new adapter. Or, if you are developing a platform that can communicate directly with LLM Engine via our REST or Websocket APIs, there is no need to develop an adapter. Add your platform the to list of supported adapters in <code>adapters/config.ts</code>.</p>"},{"location":"developers/diagrams/agent_respond_message/","title":"Agent respond message","text":"<pre><code>flowchart TD\n    Agent --&gt; MessageService\n    MessageService --&gt; Adapter\n    MessageService --&gt; WebSocketBroadcaster\n    Adapter --&gt; |\"for each configured channel\"| Recall\n    Adapter --&gt; |\"for each configured channel\"| Slack\n    Recall --&gt; Zoom\n    WebSocketBroadcaster --&gt; |\"for each configured channel\"| Nextspace\n    WebSocketBroadcaster --&gt; Nymspace\n\n    Agent[\"Agent: create response message with channels\"]\n    MessageService[\"MessageService.newMessageHandler()\"]\n    Adapter[\"Adapter.sendMessage()\"]\n    WebSocketBroadcaster[\"WebSocketBroadcaster.sendMessage()\"]\n    Recall[\"Recall\"]\n    Zoom[\"Zoom\"]\n    Slack[\"Slack\"]\n    Nextspace[\"Nextspace\"]\n    Nymspace[\"Nymspace\"]\n</code></pre>"},{"location":"developers/diagrams/new_adapter_user_creation/","title":"New adapter user creation","text":"<pre><code>graph TD\n    WS[Webhook Service] --&gt; RM[\"WebhookService.receiveMessage()\"]\n    WS --&gt; PJ[\"WebhookService.participantJoined()\"]\n\n    RM --&gt; ARM[\"Adapter.receiveMessage()\"]\n    PJ --&gt; APJ[\"Adapter.participantJoined()\"]\n\n    ARM --&gt; WS2[Webhook Service]\n    APJ --&gt; WS2\n\n    WS2 --&gt; CHK{User Exists?}\n\n    CHK --&gt;|No| CU[\"Create User: (username from adapter payload)\"]\n    CHK --&gt;|Yes| JC\n\n    CU --&gt; JC[\"ConversationService.joinConversation(user)\"]\n\n    JC --&gt; CDC{Direct ChannelsExist?}\n\n    CDC --&gt;|No| CREATE[\"Create Direct Channels: (user \u2194 agents)\"]\n    CDC --&gt;|Yes| ADD\n\n    CREATE --&gt; ADD[\"Add Direct Channel Names to Adapter Config\"]\n\n    style WS fill:#b39ddb,stroke:#333,stroke-width:2px,color:#000\n    style WS2 fill:#b39ddb,stroke:#333,stroke-width:2px,color:#000\n    style ARM fill:#ce93d8,stroke:#333,stroke-width:2px,color:#000\n    style APJ fill:#ce93d8,stroke:#333,stroke-width:2px,color:#000\n    style CU fill:#ffcc80,stroke:#333,stroke-width:2px,color:#000\n    style JC fill:#90caf9,stroke:#333,stroke-width:2px,color:#000\n    style CREATE fill:#a5d6a7,stroke:#333,stroke-width:2px,color:#000\n    style ADD fill:#fff59d,stroke:#333,stroke-width:2px,color:#000\n</code></pre>"},{"location":"developers/diagrams/receive_message_agent_routing/","title":"Receive message agent routing","text":"<pre><code>graph TD\n    MS[\"messageService.newMessageHandler()\"] --&gt; L{Route to Agents}\n    L --&gt; AG1[Agent 1]\n    L --&gt; AG2[Agent 2]\n    L --&gt; AG3[Agent N...]\n\n    AG1 --&gt; E[\"Agent.evaluate()\"]\n    AG2 --&gt; E\n    AG3 --&gt; E\n\n    E --&gt; R{Reject or Contribute?}\n    R --&gt; REJ[Reject]\n    R --&gt; CONT[Contribute]\n\n    CONT --&gt; RESP[\"Agent.respond()\"]\n\n    style MS fill:#e8f5e8\n    style AG1 fill:#fff3e0\n    style AG2 fill:#fff3e0\n    style AG3 fill:#fff3e0\n</code></pre>"},{"location":"developers/diagrams/receive_message_from_nextspace_or_nymspace/","title":"Receive message from nextspace or nymspace","text":"<pre><code>graph TD\n    N[Nextspace] --&gt; M[Message]\n    NY[Nymspace] --&gt; M\n    M --&gt; C[Websockets Message Handler]\n    M --&gt; D[\"Message API: (route and controller)\"]\n\n    C --&gt; K[\"messageService.newMessageHandler()\"]\n    D --&gt; K\n\n    style N fill:#ffecb3\n    style NY fill:#ffecb3\n    style M fill:#f0f0f0\n    style C fill:#e1f5fe\n    style D fill:#e1f5fe\n    style K fill:#e8f5e8\n</code></pre>"},{"location":"developers/diagrams/receive_message_from_zoom_or_slack/","title":"Receive message from zoom or slack","text":"<pre><code>graph TD\n    Z[Zoom] --&gt; R[Recall]\n    S[Slack] --&gt; A[Incoming Message]\n    R --&gt; A[Incoming Message]\n    A --&gt; B[\"Webhook and Handlers -&gt; (requires ngrok tunnel when run locally)\"]\n    B --&gt; C{Determine Adapter}\n    C --&gt; WS[Webhook Service]\n\n    WS --&gt; WR[\"webhookService.receiveMessage()\"]\n    WR --&gt; D[Adapter]\n\n    D --&gt; E[\"Adapter.receiveMessage()\"]\n    E --&gt; F[Create Message Object]\n    F --&gt; G[Add Configured Channels]\n\n    G --&gt; H[Audio Channel]\n    G --&gt; I[Chat Channel]\n    G --&gt; J[DM Channels...]\n\n    H --&gt; WS2[Webhook Service]\n    I --&gt; WS2\n    J --&gt; WS2\n\n    WS2 -.-&gt;|\"if new user: (see New User Flow)\"| NU[Additional Steps]\n    NU -.-&gt; K[\"MessageService.newMessageHandler()\"]\n    WS2 --&gt; K\n\n    style S fill:#ffd54f,stroke:#333,stroke-width:2px,color:#000\n    style Z fill:#ffd54f,stroke:#333,stroke-width:2px,color:#000\n    style R fill:#ffab91,stroke:#333,stroke-width:2px,color:#000\n    style B fill:#81d4fa,stroke:#333,stroke-width:2px,color:#000\n    style WS fill:#b39ddb,stroke:#333,stroke-width:2px,color:#000\n    style WS2 fill:#b39ddb,stroke:#333,stroke-width:2px,color:#000\n    style D fill:#ce93d8,stroke:#333,stroke-width:2px,color:#000\n    style K fill:#a5d6a7,stroke:#333,stroke-width:2px,color:#000\n</code></pre>"},{"location":"developers/platforms/slack/","title":"Slack","text":""},{"location":"developers/platforms/slack/#optional-set-up-slack-integration","title":"Optional: Set up Slack integration","text":"<p>LLM Engine integrates with Slack to allow agents to participate in Channel discussions or direct messages. Follow these steps to create a Slack app that can be invited to discussions in a workspace.</p>"},{"location":"developers/platforms/slack/#one-time-setup-create-the-app-in-slack","title":"One Time Setup - Create the app in Slack","text":"<p>These instructions roughly follow the steps outlined in Slack's Getting Started Guide.</p> <p>Get a free Slack Developer sandbox and then complete Step 1 in the Guide, <code>Create an App</code>. We named our app LLM Engine, but you can name yours anything.</p>"},{"location":"developers/platforms/slack/#request-scopes","title":"Request Scopes","text":"<p>Follow Step 2 <code>Requesting Scopes</code> to request the following Bot Token Scopes:</p> <ul> <li>chat:write</li> <li>chat:write.public</li> <li>channels:read</li> </ul>"},{"location":"developers/platforms/slack/#enable-messaging-from-messages-tab","title":"Enable Messaging from Messages Tab","text":"<p>Check the <code>Allow users to send Slash commands and messages from the messages tab</code> box under the <code>App Home</code> tab to allow users to DM the app.</p>"},{"location":"developers/platforms/slack/#install-and-authorize-the-app-in-your-workspace","title":"Install and Authorize the App in your Workspace","text":"<p>Follow Step 3 <code>Installing and Authorizing the App</code></p> <p>NOTE: currently you do need to invite the app to a channel in order for agents to participate</p> <p><code>/invite @LLM Engine</code> (or whatever you named the app when you created it)`</p>"},{"location":"developers/platforms/slack/#configure-the-app-for-event-listening","title":"Configure the app for event listening","text":"<p>Follow Step 4 <code>Configuring the app for event listening.</code> Set the request URL to the following address of your running LLM Engine server: <code>https://[base URL]/v1/webhooks/slack</code></p> <p>Subscribe to the following bot events</p> <ul> <li>message.channels</li> <li>message.groups</li> <li>message.im</li> </ul>"},{"location":"developers/platforms/slack/#add-environment-variables","title":"Add environment variables","text":"<p>Set the following environment variable in the LLM Engine <code>.env</code> file:</p> <pre><code>SLACK_SIGNING_SECRET - found under Basic Information in your Slack app configuration\n</code></pre>"},{"location":"developers/platforms/slack/#using-slack-with-llm-engine","title":"Using Slack with LLM Engine","text":"<ol> <li>Ensure ngrok tunnel is running or LLM Engine is otherwise remotely accessible.</li> <li>Determine your workspace ID. It is the last part of the URL when you select the workspace, starting with T.</li> <li>Determine the Slack Channel ID. It is the last part of the URL when you select the channel in Slack.</li> <li>Copy your Bot User OAuth Token under OAuth &amp; Permissions in your Slack app configuration for your workspace. Each workspace has a unique bot token. NOTE: make sure you copy the Bot token and not the User token.</li> <li>Create a <code>Conversation</code> with the desired channels and provide the Slack Channel ID, Workspace ID, and Bot Token in the Slack <code>adapter config</code></li> </ol> <p>Example conversation body:</p> <pre><code>{\n    \"name\": \"Should plastic water bottles be banned?\",\n    \"topicId\": \"{{defaultTopic}}\",\n    \"channels\": [ { \"name\": \"moderator\"}, { \"name\": \"participant\"}],\n    \"adapters\": [ {\"type\": \"slack\", \"config\" : {\"channel\": \"C08US6FL6DV\", \"workspace: \"T123494\", botToken:[token],\n        \"chatChannels\": [ { \"name\": \"playfulSlack\", \"direction\": \"both\"}]}}]\n}\n</code></pre> <p>NOTE: A unique Slack workspace and channel combination can only be associated with one active Conversation</p> <ol> <li>Post a message to the Slack Channel. Any agents configured on the <code>Conversation</code> should receive and send messages on their typical channels.</li> </ol> <p>If you wish to support DMs between users and agents, you must configure a separate Conversation for all DMs. You can only have one such Conversation active at a time. Private communication between a user and the Slack app happens on a dedicated channel (different for each user). Therefore, you must use the keyword 'direct' for channel name, to signal that the conversation should process all direct messages.</p> <p>Example conversation body:</p> <pre><code>{\n    \"name\": \"A chat\",\n    \"topicId\": \"{{defaultTopic}}\",\n    \"enableDMs\": ['agents'],\n    \"agentTypes\": [agents],\n    \"adapters\": [ {\"type\": \"slack\", \"config\" : {\"channel\": \"direct\", \"workspace: \"T123494\", botToken:[token],\n        \"dmChannels\": [{ \"direct\": true, \"agent\": \"playfulPerMessage\", \"direction\": \"both\"}]}}]\n}\n</code></pre>"},{"location":"installing/","title":"Installing LLM Engine","text":""},{"location":"installing/#configuration","title":"Configuration","text":"<p>Configuration is done with an environment variables file. Copy <code>.env.example</code> to create your <code>.env</code> file.</p>"},{"location":"installing/#running-locally","title":"Running locally","text":"<ol> <li>Start by copying <code>.env.example</code> to <code>.env</code>.</li> <li>Install <code>mongodb</code> (ubuntu 24 server)</li> <li>Run MongoDB with <code>mongod</code> <p>\ud83d\udca1 Note: Mac users who have used Homebrew to install MongoDB should use the command <code>brew services start mongodb-community</code> to run the MongoDB service instead.</p> </li> <li>Install <code>node.js</code> and set to a version specified in <code>package.json</code> file (Consider using nvm)</li> <li>Install yarn</li> <li>Install all dependencies with <code>yarn install</code>.</li> <li>Run <code>yarn run dev</code> to serve the API locally.</li> </ol>"},{"location":"installing/#llm-model-selection","title":"LLM Model selection","text":"<p>LLM Engine supports a range of LLM platforms.</p>"},{"location":"installing/#openai","title":"OpenAI","text":"<ol> <li>Configure <code>DEFAULT_OPENAI_API_KEY</code> and <code>DEFAULT_OPENAI_BASE_URL</code> in your <code>.env</code> file.</li> <li>When creating a Conversation with an Agent, specify <code>llmPlatform</code> to be <code>openai</code> and <code>llmModel</code> to be an available OpenAI model.</li> </ol> <p>Note that this will work for any OpenAI compatible LLM provider.</p>"},{"location":"installing/#aws-bedrock-including-claude","title":"AWS Bedrock (including Claude)","text":"<ol> <li>Configure <code>BEDROCK_API_KEY</code> and <code>BEDROCK_BASE_URL</code> in your <code>.env</code> file.</li> <li>When creating a Conversation with an Agent, specify <code>llmPlatform</code> to be <code>bedrock</code> and <code>llmModel</code> to be an available Bedrock model.</li> </ol>"},{"location":"installing/#open-source-models-via-vllm","title":"Open Source Models via vLLM","text":"<p>Open source models are available through vLLM running locally or on one of two hosted serverless providers:</p> <ul> <li>Runpod - See detailed instruction for setup in our runpod guide.</li> <li>Modal - See detailed instructions for setup in our modal guide.</li> <li> <p>Local vLLM - Follow their setup guide.</p> </li> <li> <p>Configure <code>VLLM_API_KEY</code> and <code>VLLM_BASE_URL</code> in your <code>.env</code> file.</p> </li> <li>When creating a Conversation with an Agent, specify <code>llmPlatform</code> to be <code>vllm</code> and <code>llmModel</code> to be an available open source model supported by vllm.</li> </ul>"},{"location":"installing/#open-source-models-via-ollama","title":"Open Source Models via Ollama","text":"<p>Open source models are also available through Ollama running locally.</p> <ol> <li>Install Ollama locally.</li> <li>Configure <code>OLLAMA_BASE_URL</code></li> <li>When creating a Conversation with an Agent, specify <code>llmPlatform</code> to be <code>ollama</code> and <code>llmModel</code> to be an available open source model supported by ollama.</li> </ol>"},{"location":"installing/#optional-retrieval-augmented-generation","title":"Optional: Retrieval Augmented Generation","text":"<p>If you would like to make use of Retrieval Augmented Generation (RAG) see our rag guide.</p>"},{"location":"installing/#optional-nextspace-integration","title":"Optional: Nextspace integration","text":"<p>If you would like to use LLM Engine with the Nextspace client, see our nextspace guide.</p>"},{"location":"installing/#optional-zoom-integration","title":"Optional: Zoom integration","text":"<p>If you would like to use LLM Engine with Zoom, see our zoom guide.</p>"},{"location":"installing/modal/","title":"Modal model serving","text":""},{"location":"installing/modal/#overview","title":"Overview","text":"<p>We support usng Modal to host open source models on a vLLM container, served as a web endpoint.</p> <p>vLLM is attractive because it provides an OpenAI compatible API for all models it supports. That allows us to use the standard OpenAI classes of LangChain.</p>"},{"location":"installing/modal/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Modal.com account</li> <li>Modal CLI installed (<code>pip install modal</code>)</li> <li>Modal CLI authenticated (<code>modal setup</code> or <code>python -m modal setup</code> to generate a token)</li> </ol>"},{"location":"installing/modal/#billing","title":"Billing","text":"<p>You sign up for a plan that gives you a certain dollar amount of included credits per month. You can (and should!) set a maximum monthly budget for pay-as-you-go once you exceed the allowable credits in a month.</p>"},{"location":"installing/modal/#serverless-configuration","title":"Serverless Configuration","text":"<p>Modal is a cloud function platform that provides full serverless execution by allowing you to serve functions as web endpoints. Everything is defined and configured in code and deployed via command line. Therefore, you will need code to select a container image, download a model, and configure, build and serve vLLM. We use a sample Python script provided by Modal for this in the setup steps below.</p>"},{"location":"installing/modal/#models-and-endpoints","title":"Models and Endpoints","text":"<p>Right now, each different model you want to use will require configuring a different Modal function (and possibly App).</p>"},{"location":"installing/modal/#setup-steps","title":"Setup steps","text":"<p>To get up and running quickly, we will follow this example from Modal to deploy an OpenAI-compatible LLM service with vLLM and the RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8 model. Please consult the example page for details.</p> <ol> <li>Clone the modal examples repository and deploy the vLLM server (NOTE: this creates an unsecure endpoint by default. See Modal documentation for details on how to modify the script to secure the endpoint)</li> </ol> <pre><code>$ git clone https://github.com/modal-labs/modal-examples\n$ cd modal-examples/06_gpu_and_ml/llm-serving/\n$ modal deploy vllm_inference.py\n</code></pre> <ol> <li>Navigate to the Modal URL provided by the script and verify your function is up.</li> <li>Configure the vLLM variables in our .env file accordingly to connect</li> </ol> <pre><code>VLLM_API_URL=https://{prefix}--example-vllm-inference-serve.modal.run/v1\n# if using the default function with no security\nVLLM_API_KEY='dummy'\n</code></pre>"},{"location":"installing/modal/#cold-starts-and-scaling","title":"Cold starts and Scaling","text":"<p>Modal performs auto-scaling of functions. They scale down to 0 when not in use (default idle timeout is 15 minutes).</p> <p>When an endpoint is \"idle\" it will need to cold start. This seems to take about 90 seconds with the example vLLM server. As a result you may want to send a \"wake up\" request when first starting an agent conversation to get the endpoint ready. Our code can do this (see the <code>llmPlatforms</code> configuration data in the <code>getModelChat</code> file, specifically the <code>useKeepAlive</code> value). We also issue a wake up ping before starting test runs against a vLLM platform.</p> <p>See Modal documentation for additional tips on optimizing cold start time.</p>"},{"location":"installing/modal/#sanity-testing","title":"Sanity Testing","text":"<p>You can use curl to test your endpoint's OpenAI compatible API. Adjust the <code>PREFIX</code> and <code>VLLM_API_KEY</code> and model accordingly. Just remember the first request may take a while while the endpoint is brought online.</p> <pre><code>curl -X POST https://{PREFIX}--example-vllm-inference-serve.modal.run/v1/chat/completions \\\n -H 'Content-Type: application/json' \\\n -H 'Authorization: Bearer VLLM_API_KEY' \\\n -d '{\n\"model\": \"RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8\",\n\"messages\": [\n{\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n]\n}'\n</code></pre>"},{"location":"installing/modal/#keep-alive","title":"Keep Alive?","text":"<p>We have not yet implemented keep alive pings during the duration of a conversation. That would be an alternative to setting a long idle timeout. Managing the pings to run only when any conversation is running could be done, but would take a little work.</p>"},{"location":"installing/rag/","title":"Rag","text":""},{"location":"installing/rag/#optional-set-up-chroma-to-use-rag","title":"Optional: Set up Chroma to use RAG","text":"<p>We use Chroma as the vector store. If you are using RAG, you can start Chroma using a Docker Container. NOTE We are currently compatible only with Chroma 0.6.3. You must use that version.</p> <pre><code>docker pull chromadb/chroma\ndocker run -p 8000:8000 chromadb/chroma:0.6.3\n</code></pre> <p>If you wish to investigate running Chroma in the cloud, see Chroma documentation for further details.</p>"},{"location":"installing/rag/#setting-an-embeddings-provider","title":"Setting an embeddings provider","text":"<p>You may specify an OpenAI compatible embeddings provider using these environment variables: <pre><code>EMBEDDINGS_API_URL=\nEMBEDDINGS_API_KEY=\nEMBEDDINGS_DOCUMENT_MODEL=\nEMBEDDINGS_REALTIME_MODEL=\n</code></pre></p> <p>If you don't provide them, a standard OpenAI embeddings model will be used.</p> <p>You can use these settings to set up a private, open source embeddings model if you wish.</p> <p>We have tested this with the <code>AAI/bge-small-en-v1.5</code> embeddings model using Infinity hosted on Runpod.io.</p> <p>Please see Runpod instruction for more information.</p>"},{"location":"installing/runpod/","title":"Runpod model serving","text":""},{"location":"installing/runpod/#overview","title":"Overview","text":"<p>We support usng Runpod.io to host open source models on a vLLM container. In particular, we host a serverless endpoint running a vLLM container provided by a standard template.</p> <p>vLLM is attractive because it provides an OpenAI compatible API for all models it supports. That allows us to use the standard OpenAI classes of LangChain.</p>"},{"location":"installing/runpod/#prerequisites","title":"Prerequisites","text":"<ol> <li>A runpod.io account</li> <li>A credit card attached to the account</li> <li>Also create a HuggingFace account</li> <li>Accept license terms for an open model you plan to use on HuggingFace</li> <li>Generate an API token on HuggingFace, which will be used to pull models</li> </ol>"},{"location":"installing/runpod/#billing","title":"Billing","text":"<p>You attach a credit card and pre-pay for a certain value of credits. You can enable or disable automatic renewals when the balance drops below a certain level, but be careful of this to avoid surprise bills!</p>"},{"location":"installing/runpod/#serverless-configuration","title":"Serverless Configuration","text":""},{"location":"installing/runpod/#models-and-endpoints","title":"Models and Endpoints","text":"<p>Right now, each different model you want to use will require configuring a different endpoint. THe vLLM templates provided by Runpod support only one model at a time. vLLM itself can support multiple models, but to use that functionality you would need to setup your own template.</p>"},{"location":"installing/runpod/#setup-steps","title":"Setup steps","text":"<ol> <li>Create a new serverless endpoint and select the latest vLLM template.</li> <li>Specify a model from those supported and set your HuggingFace token. (Make sure you have acceepted the model terms first!)</li> <li>Select a data center that has a good supply of the GPUs you prefer to use. Look for \"PRO\" GPUs which will be more reliable. Select only that data center.</li> <li>Create a network volume in that data of sufficient size to hold the model and other vLLM files.</li> <li>Ensure the vLLM environment variables have <code>BASE_PATH=/runpod-volume</code> set so that the volume is used for storing the models.</li> <li>Attach that network volume to your endpoint, so that once the model is download, it does not need to be downloaded again. This will reduce cold start times.</li> <li>Generate a runpod API token</li> <li>Configure the vLLM variables in our .env file accordingly</li> </ol> <pre><code>VLLM_API_URL=https://api.runpod.ai/v2/{endpointId}/openai/v1\nVLLM_API_KEY={token}\n</code></pre>"},{"location":"installing/runpod/#cold-starts-and-scaling","title":"Cold starts and Scaling","text":"<p>A serverless endpoint allows us to set up scale up and scale down efficiently and cost effectively. We scale down to 0 when not in use. THe endpoint queues requests automatically.</p> <p>When an endpoint is \"idle\" it will need to cold start. This seems to take about 2 minutes. As a result you may want to send a \"wake up\" request when first starting an agent conversation to get an endpoint worker ready. Our code can do this (see the <code>llmPlatforms</code> configuration data in the <code>getModelChat</code> file, specifically the <code>useKeepAlive</code> value). We also issue a wake up ping before starting test runs against runpod.</p> <p>Once the worker is running, then requests will be queued and processed immediately.</p> <p>Runpod also has a \"FastBoot\" option which can bring most cold start requests down to a couple seconds. But this depends on competition for FastBoot caching on runpod and may be higher at times.</p> <p>Our initial scaling configuration: * Max workers: 5 * Active workers: 0 * GPU count: 1 * Idle timeout: 120 sec * Execution timeout (of one request): 600 sec * Enable Flashboot: YES! Makes cold starts much faster * Model: mistralai/Mistral-7B-Instruct-v0.3 * Queue delay: 15 secs (A new worker will be created if allowed when a request is in queue longer than this)</p> <p>Environment variables: * MAX_CONCURRENCY=8 * GPU_MEMORY_UTILIZATION=0.90</p> <p>Note: This may be insufficient for large events or events with frequent LLM calls. In that case, add workers.</p> <p>Note: If you update the endpoint, delete \"outdated\" workers using the Runpod.io console.</p>"},{"location":"installing/runpod/#sanity-testing","title":"Sanity Testing","text":"<p>You can use curl to test your endpoint's OpenAI compatible API. Adjust the <code>ENDPOINT_ID</code> and <code>VLLM_API_KEY</code> and model accordingly. Just remember the first request may take a while while the endpoint is brought online.</p> <pre><code>curl -X POST https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer VLLM_API_KEY' \\\n  -d '{\n    \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n  }'\n</code></pre>"},{"location":"installing/runpod/#keep-alive","title":"Keep Alive?","text":"<p>We have not yet implemented keep alive pings during the duration of a conversation. That would be an alternative to setting a long idle timeout. Managing the pings to run only when any conversation is running could be done, but would take a little work. With FastBoot and a reasonable idle timeout, hopefully that is not needed.</p>"},{"location":"installing/runpod/#embeddings","title":"Embeddings","text":"<p>You can use Runpod to host embeddings as well using the official Infinity Embeddings serverless template.</p> <p>A 16 GB GPU should be sufficient. As with vllm hosting, you will improve latency if you attach network storage.</p> <p>We have used: * Max workers: 1 (or 2) * Idle timeout: 120 sec * Enable Flashboot * Execution timeout (of one request): 600 sec * Model: <code>AAI/bge-small-en-v1.5</code></p> <p>With environment variables: <pre><code>RUNPOD_MAX_CONCURRENCY = 50\n</code></pre></p>"},{"location":"platforms/nextspace/","title":"Nextspace","text":""},{"location":"platforms/nextspace/#nextspace-integration","title":"Nextspace integration","text":"<p>LLM Engine integrates with Nextspace!</p> <p>Please visit that project to see how to set it up for use with LLM Engine.</p>"},{"location":"platforms/zoom/","title":"Zoom","text":""},{"location":"platforms/zoom/#zoom-integration","title":"Zoom integration","text":"<p>LLM Engine can integrate with Zoom! It supports:</p> <ul> <li>live transcription</li> <li>sending and receiving direct messages</li> </ul> <p>Zoom integration requires the use of Recall.ai, a third party service.</p>"},{"location":"platforms/zoom/#set-up-recallai","title":"Set up Recall.ai","text":"<p>We provide a Zoom adapter that saves real-time transcription of a Zoom meeting to LLM Engine. We use the meeting bot API provided by Recall.ai to connect to Zoom. Follow these steps to integrate with Recall:</p>"},{"location":"platforms/zoom/#one-time-setup","title":"One Time Setup","text":"<ol> <li>Complete Step 1 of these instructions to create a Zoom marketplace app and connect it to Recall.</li> <li>In Zoom Marketplace, Set up Zoom event subscriptions in your app to receive meeting started and ended events. Choose Webhook method and set the event notification endpoint URL to [baseUrl]/v1/webhooks/zoom</li> <li>Set the four Recall environment variables and Zoom event subscription variable in <code>.env</code></li> </ol> <pre><code>RECALL_API_KEY\nRECALL_BASE_URL: the URL used to connect to your Recall account\nRECALL_TOKEN: can be anything. Sent to Recall to use as a query param for authentication when it calls back to our webhooks\nRECALL_BASE_URL: The URL Recall should use to send events to this server, e.g. a static ngrok domain if running locally\nZOOM_SECRET_TOKEN: the secret token found in your app configuration in the Zoom marketplace\n</code></pre>"},{"location":"platforms/zoom/#zoom-webinar-one-time-setup","title":"Zoom Webinar One Time Setup","text":"<p>Recall bots can join Zoom webinars as well as meetings, but they must be added to the webinar as panelists. To enable this, create a dedicated email address for your bots. Then update this environment variable in <code>.env</code></p> <pre><code>ZOOM_WEBINAR_USER_EMAIL=[bot email address]\n</code></pre>"},{"location":"platforms/zoom/#using-llm-engine-in-a-zoom-meeting","title":"Using LLM Engine in a Zoom Meeting","text":"<ol> <li>Ensure ngrok tunnel is running (for local development) or LLM Engine is otherwise remotely accessible from the wider internet.</li> <li>Schedule a Zoom meeting and copy the full invite link, including any passcodes and meeting IDs.</li> <li>Create a <code>Conversation</code> with a <code>transcript</code> channel and provide the <code>meetingUrl</code> in the Zoom <code>adapter config</code>.</li> </ol> <p>Example conversation body:</p> <pre><code>{\n    \"name\": \"Should plastic water bottles be banned?\",\n    \"topicId\": \"{{defaultTopic}}\",\n    \"channels\": [ { \"name\": \"transcript\"}],\n    \"adapters\": [ {\"type\": \"zoom\", \"config\" : {\"meetingUrl\": \"{{ZOOM_MEETING_URL}}\"}}]\n}\n</code></pre> <p>NOTE: A unique Zoom meeting URL can only be associated with one active Conversation</p>"},{"location":"platforms/zoom/#zoom-webinars","title":"Zoom Webinars","text":"<p>You must add the bot as a panelist when scheduling a webinar in order to allow recording. Invite the bot to the webinar as a panelist using the email address you created in the one-time setup. Be sure to use the specific Zoom invite link sent to the panelist bot as the <code>meetingUrl</code>. You can get this link by copying the invitation when you add the panelist. If your webinar requires registration, the bot user does NOT have to register.</p> <p>Note: Webinar participants in a Zoom session cannot DM a single panelist, so agents that interact through DMs in Zoom meetings will not function the same in a webinar.</p>"},{"location":"platforms/zoom/#data-retention","title":"Data Retention","text":"<p>By default, Zoom meeting recordings are retained on the Recall server for one hour after the meeting. You can specify a different retention policy in your adapter config. Set retention to null for zero data retention (but note, this will impact your ability to debug Recall issues). Otherwise, see Recall documentation for supported options.</p> <p>Example setting data retention to four hours:</p> <pre><code>{\n    \"name\": \"Should plastic water bottles be banned?\",\n    \"topicId\": \"{{defaultTopic}}\",\n    \"channels\": [ { \"name\": \"transcript\"}],\n    \"adapters\": [ {\"type\": \"zoom\", \"config\" : {\"meetingUrl\": \"{{ZOOM_MEETING_URL}}\",     \"retention\": {\"type\": \"timed\",hours: 4}}, \"audioChannels\": [{\"name\": \"transcript\"}]}]\n}\n</code></pre> <ol> <li>Start your Zoom meeting. After a minute or two, LLM Engine should join the meeting and ask for recording permission.</li> <li>Grant permission to record and real-time transcription should start.</li> </ol>"},{"location":"platforms/zoom/#enabling-direct-messages","title":"Enabling Direct Messages","text":"<p>If you wish to enable direct messages between agents and users, you must set the <code>enableDMs\": [\"agents\"]</code> property during Conversation creation. You must also specify a <code>dmChannel</code> in the <code>Adapter</code> configuration for each agent that should receive DMs.</p> <pre><code>{\n\"name\": \"Should plastic water bottles be banned?\",\n\"topicId\": \"{{defaultTopic}}\",\n\"enableDMs\": [\"agents\"]\n\"channels\": [ { \"name\": \"transcript\"}],\n\"adapters\": [ {\"type\": \"zoom\", \"config\" : {\"meetingUrl\": \"{{ZOOM_MEETING_URL}}\"},\n    \"dmChannels\": [{\"direct\": true, \"agent\": \"eventAssistant\", \"direction\": \"both\"}],\n    \"audioChannels\": [{\"name\": \"transcript\"}]}]\n}\n</code></pre> <p>If you do not need transcription, remove the <code>transcript</code> channel from the configuration.</p>"},{"location":"using/","title":"Using LLM Engine","text":""},{"location":"using/#creating-a-conversation","title":"Creating a Conversation","text":"<p>Once you have LLM Engine up and running, the first thing to do is create a Conversation. Since LLM Engine is a headless service, you can interact with the <code>conversations</code> API through any REST client such as Postman or directly through the OpenAPI swagger endpoint on your locally running server, at http://localhost:3000/v1/docs. If you wish to use a Front End to create and configure a Conversation, we recommend our open source Nextspace app.</p> <p>The easiest way to create a conversation is to use the <code>/conversations/from-type</code> endpoint, which allows you to specify only the minimal properties needed to create a conversation that uses either our Event Assistant or Back Channel production agents.</p> <p>Here is an example conversation that uses the Event Assistant:</p> <pre><code>{\n    \"name\": \"Where are all the aliens?\",\n    \"topicId\": \"{{topicId}}\",\n    \"type\": \"eventAssistant\",\n    \"platforms\": [\"zoom\"],\n    \"properties\": {\"zoomMeetingUrl\": \"{{ZOOM_MEETING_URL}}\"}\n}\n</code></pre> <p>Note: These agents require Zoom for transcription, so you must provide a valid Zoom Meeting URL. See our Zoom guide for more information.</p>"},{"location":"using/#monitoring-a-conversation","title":"Monitoring a Conversation","text":"<p>If you would like to monitor or run reports on agent responses and performance during a conversation, see our Monitoring and Reporting guide.</p>"},{"location":"using/#experimenting-with-agents","title":"Experimenting with Agents","text":"<p>If you would like to experiment with an existing agent by modifying its prompts or changing its model, see our Experimentation guide.</p>"},{"location":"using/experiments/","title":"Experiments","text":"<p>LLM Engine provides a basic structure for running experiments against past conversations. Experiments are most useful in three situations:</p> <ol> <li>You have modified the code of an agent and want to see how it now performs against a past conversation</li> <li>You want to change something that is typically configured as a an agent property, such as the prompt or the model the agent uses (i.e. seeing how GPT-4o-mini might compare to GPT-4.1)</li> <li>(not yet supported) You want to test a brand new agent against a past conversation</li> </ol>"},{"location":"using/experiments/#running-an-experiment-against-updated-agent-code","title":"Running an experiment against updated agent code","text":"<p>If you updated an agent that was previously used in a conversation, you can easily rerun the conversation through the new agent code and view the new responses. Use a retrieval endpoint to find the conversation and agent IDs, then POST the following body to <code>/experiments/</code></p> <pre><code>{\n    \"name\": \"My Event\",\n    \"baseConversation\": \"{{conversationId}}\",\n    \"agentModifications\": [\n        {\n            \"agent\": \"{{agentId}}\",\n        }\n    ]\n}\n</code></pre>"},{"location":"using/experiments/#running-an-experiment-with-experimental-agent-property-values","title":"Running an experiment with experimental agent property values","text":"<p>To modify agent property values like prompt or llmModel, you can create an experiment with <code>experimentValues</code> properties. Use a retrieval endpoint to find the conversation and agent IDs, then POST the following body to <code>/experiments/</code></p> <pre><code>{\n    \"name\": \"My Event\",\n    \"baseConversation\": \"{{conversationId}}\",\n    \"agentModifications\": [\n        {\n            \"agent\": \"{{agentId}}\",\n            \"experimentValues\": {\n                \"llmTemplates\": {\n                    \"system\": \"Do something different than the last prompt I gave you\",\n                },\n                \"llmPlatform\": \"openai\",\n                \"llmModel\": \"gpt-4o-mini\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"using/experiments/#viewing-experiment-results","title":"Viewing experiment results","text":"<p>See [LLM Engine Reports] (./monitoring.md) for instructions on running a report to view the results of the experiment.</p>"},{"location":"using/monitoring/","title":"Monitoring and Reporting","text":"<p>LLM Engine provides some support for monitoring a live event and generating post-event reports for additional insight into agent performance.</p>"},{"location":"using/monitoring/#langsmith-integration","title":"Langsmith integration","text":"<p>We integrate with Langsmith for real-time conversation tracing. To connect and use Langsmith, set the following environment variables in your <code>.env</code> file:</p> <pre><code>LANGSMITH_TRACING_V2=true\nLANGSMITH_API_KEY=[Your API key]\n# Optional\nLANGSMITH_PROJECT=[any unique identifier]\n</code></pre> <p>When <code>LANGSMTIH_TRACING_V2</code> is set to true, Langsmith will report all agent interactions as traces in the Langsmith dashboard. You can inspect these traces to see user input and agent responses, as well as the context and conversation history that was provided to the agent. Langsmith also reports latency and token usage.</p> <p>Traces can be manually annotated with feedback or configured to be automatically annotated with evaluators. Importantly, traces can be added to a dataset to support future evaluation and experimentation.</p>"},{"location":"using/monitoring/#llm-engine-reports","title":"LLM Engine reports","text":"<p>LLM Engine offers an endpoint to generate summary reports of a past conversation, including basic statistics like average number of interactions and a detailed history of all user and agent messages. The reporting endpoint is linked to the experimentation feature, but it is possible to label a past conversation as an experiment in order to generate a report.</p>"},{"location":"using/monitoring/#creating-an-experiment-from-a-past-conversation","title":"Creating an experiment from a past conversation","text":"<p>To create an experiment from a conversation, post the following body to the <code>/experiments</code> endpoint:</p> <pre><code>{\n  \"name\": [conversation name],\n  \"baseConversation\": [conversation id],\n  \"executedAt\": [conversation start time in ISO8601 format, e.g. \"2025-11-19T17:20:00.000Z\" ]\n}\n</code></pre> <p>The response will contain the ID of the new experiment, which can be used to generate a report</p>"},{"location":"using/monitoring/#running-a-report","title":"Running a report","text":"<p>There are two types of reports for conversations: direct message and periodic.</p>"},{"location":"using/monitoring/#direct-message-reports","title":"Direct message reports","text":"<p>The direct message report is for agents like the Event Assistant that are used for direct user to agent communication only. The report prints the message history for each user interaction in the conversation. To run the report, make the following GET request: <code>/experiments/{{experimentId}}/results?reportName=directMessageResponses&amp;format=text</code></p> <p>The response should look something like this:</p> <pre><code>Direct Message Agent Responses Report\n===========================\n\nExperiment: My Event\nExperiment Time: 11/19/2025, 5:20:00 PM\nBase Conversation ID: 123456789\nResult Conversation ID: 123456789\n\n===========================\nAgent Name: Event Assistant\nTotal Users Messaged: 68\nTotal Users Responded: 32\nMin Engagements Per User: 1\nMax Engagements Per User: 24\nAverage Engagements Per User: 4.09375\n\n---------------------------\n**User: Steely Angelfish**\n\n5:21:01 PM  Event Assistant: Hi! I'm the LLM Event Assistant. If you miss something, or want a clarification on something that\u2019s been said during the event, you can DM me. None of your messages to me will be surfaced to the moderator or the rest of the audience.\n\n5:21:05 PM  Steely Angelfish: What is this event about?\n\n...\n\n---------------------------\n---------------------------\n**User: Scholarly Babbler**\n\n5:35:51 PM  Event Assistant: Hi! I'm the LLM Event Assistant. If you miss something, or want a clarification on something that\u2019s been said during the event, you can DM me. None of your messages to me will be surfaced to the moderator or the rest of the audience.\n\n5:42:19 PM  Scholarly Babbler: I just joined. What have I missed?\n\n...\n</code></pre>"},{"location":"using/monitoring/#periodic-reports","title":"Periodic reports","text":"<p>The periodic report is for agents like the Back Channel that generate responses periodically, rather than on-demand in response to a user message. The report prints all user messages and agent responses within each time interval. To run the report, make the following GET request: <code>/experiments/{{defaultExperiment}}/results?reportName=periodicResponses&amp;format=text</code></p> <p>The response should look something like this:</p> <pre><code>Periodic Agent Responses Report\n===========================\n\nExperiment: My Moderated Event\nGenerated: 9/10/2025, 12:20:00 PM\nUnique Participants: 22\n\n===========================\nAgent Name: Back Channel Insights Agent\n\n***Messages in time period: 12:31:27 PM - 12:33:27 PM***\n\n12:32:39 PM  Blithe Belemnite: What is the speaker's definition of artificial intelligence?\n\n\n**Agent Responses**\n\n  Insights:\n\n      Value: What is the speaker's definition of artificial intelligence?\n      Comments:\n\n          User: Blithe Belemnite\n          Text: What is the speaker's definition of artificial intelligence?\n\n...\n</code></pre> <p>More report types and formats other than text may be developed in the future.</p>"}]}